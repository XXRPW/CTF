<h1>Training: WWW-Robots(HTTP, Training)</h1>

<br><br>

challenge:
In this little training challenge, you are going to learn about the Robots_exclusion_standard.
The robots.txt file is used by web crawlers to check if they are allowed to crawl and index your website or only parts of it.
Sometimes these files reveal the directory structure instead protecting the content from being crawled.

Enjoy!

It tells you a little bit of a robot file and the vulnerbility of it. 
if you go to the wiki and read a little you will know that robots file are located at root which will be wechall.net/robots.txt
so if you type that in you will goto a page that says
User-agent: *
Disallow: /challenge/training/www/robots/T0PS3CR3T


User-agent: Yandex
Disallow: *

and if you copy and paste wechall.net/challenge/training/www/robots/T0PS3CR3T
you solve the challenge